# -*- coding: utf-8 -*-
"""DontGetKicked.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11SRN8ilTUvkhQM06uXrU9qUKRAOOSFrh
"""

# The used dataset can be found here(training.csv): 
# https://www.kaggle.com/c/DontGetKicked/data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from random import randrange

# Data preparation
from sklearn_pandas import CategoricalImputer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
# from imblearn.over_sampling import SMOTE
# from imblearn.combine import SMOTEENN

# Classifiers
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, \
  VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier

# Metrics
from sklearn.metrics import roc_curve, classification_report

# Cross-validation, Misc
from sklearn.model_selection import train_test_split, StratifiedKFold, \
  cross_val_score, cross_val_predict, GridSearchCV
from sklearn.pipeline import Pipeline

DATA = 'training.csv'
TEST_SIZE = 0.3
KFOLD_SPLITS = 10
SEED = 7
SCORE_ACC = 'accuracy'
SCORE_AUC = 'roc_auc'
MAX_ITER = 1000

# Loading data..
data = pd.read_csv(DATA)
data.head()

# Getting to know our data..
data.dtypes

# We notice that we have an unbalanced labeled dataset.
data.groupby("IsBadBuy").size()

# Seperating target attribute from the rest of the data..
target = data.iloc[:, 1]
data.drop("IsBadBuy", axis = 1, inplace = True)

# Finding redundant data: attributes with high amount of missing values
print('datacount:', len(data))
print('Nan in data:\n', data.isnull().sum())

# Finding redundant data: highly correlated attributes
corr = data.corr()
corr.style.background_gradient(cmap='coolwarm')

# The purchase date attribute is too specific, but the month part seems
# to have some significance over our bad buys. Thus, we extract it into
# a seperate feature and we will discard the old one.
data['MonthOfPurchase'] = pd.DatetimeIndex(data['PurchDate']).month

# Dropping: a) Attributes not providing actual information.
#           b) Attributes with high missing values.
#           c) Some of the highly correlated attributes.
data.drop(["RefId", "PurchDate", "VehYear", "Model", \
  "SubModel", "WheelType", \
  "PRIMEUNIT", "AUCGUART"], axis = 1, inplace = True)

# Imputing categorical columns with most frequent values..
categorical_feature_mask = data.dtypes==object
categorical_cols = data.columns[categorical_feature_mask].tolist()
catImputer = CategoricalImputer(strategy='most_frequent')
for col in categorical_cols:
  data[col] = catImputer.fit_transform(data[col])

# Imputing numerical columns with median values. Before imputation,
# we have to change the datatype from Float64 to int.
numerical_cols = data.columns.drop(categorical_cols).tolist()
data[numerical_cols] = data[numerical_cols].fillna(-1)
data[numerical_cols] = data[numerical_cols].astype(np.int64)
data[numerical_cols] = data[numerical_cols].replace(-1, np.nan)
imputer = SimpleImputer(missing_values = np.nan, strategy='median')
data[numerical_cols] = imputer.fit_transform(data[numerical_cols])

# One-hot encoding categorical data to dummy attributes..
data = pd.get_dummies(data[categorical_cols])

# Standardizing our data, so as to follow normal distribution with 
# zero mean and unit variance. This primarily helps when applying 
# Logistic Regression.
scaler = StandardScaler()
data = scaler.fit_transform(data)

# We have to balance our dataset. We are going to achieve this by downsampling
# the good buys.
xBad = data[target == 1] 
yBad = target[target == 1] 
xGood = data[target == 0]
yGood = target[target == 0]
print('Before balancing:\n\tGood buys(0): {}, '\
  'Bad buys(1): {}'.format(len(xGood), len(xBad)))
xGoodBalanced = xGood[:len(xBad), :]
yGoodBalanced = yGood[:len(xBad)]
print('After balancing:\n\tGood buys(0): {}, '\
  'Bad buys(1): {}'.format(len(xGoodBalanced), len(xBad)))
dataBalanced = np.concatenate((xGoodBalanced, xBad), axis=0)
targetBalanced = np.concatenate((yGoodBalanced, yBad), axis=0)

# Alternative strategy: Oversample the bad buys
# sm = SMOTE(ratio = 'minority')
# dataBalanced, targetBalanced = sm.fit_sample(data, target)

# Seperating training-validating from testing dataset
xTrain_Val, xTest, yTrain_Val, yTest = train_test_split(dataBalanced, \
  targetBalanced, test_size = TEST_SIZE, random_state = SEED)

# Initializing k-fold cross validation model
kfold = StratifiedKFold(n_splits = KFOLD_SPLITS, shuffle = True, \
  random_state = SEED)
clfs = []

def produceROCs():
  for clf in clfs:
    hlp = cross_val_predict(clf['clf'], xTrain_Val, yTrain_Val, \
      method = 'predict_proba', n_jobs = -1)
    fpr, tpr, thresholds = roc_curve(yTrain_Val, hlp[:, 1])
    plt.plot(fpr, tpr, label = clf['label'])
  plt.plot([0, 1], [0, 1], 'k--')
  plt.style.use('fivethirtyeight')
  plt.title("ROC Curves")
  plt.xlabel("False Positive Rate")
  plt.ylabel("True Positive Rate")
  plt.legend(loc='best')
  plt.show()

# Proceeding to validation phase..
# First, we are going to use Logistic Regression, with many different C values.
pipelineEstimator = Pipeline([('clf', \
  LogisticRegression(max_iter = MAX_ITER, random_state = SEED))])
c_space = np.logspace(-5, 1, 10)
parameters = [{'clf__C': c_space}]
grid_obj = GridSearchCV(estimator = pipelineEstimator, \
  param_grid = parameters, scoring = SCORE_ACC, n_jobs = -1)
grid_obj.fit(xTrain_Val, yTrain_Val)

print(pd.concat([pd.DataFrame(grid_obj.cv_results_["params"]), \
  pd.DataFrame(np.round(grid_obj.cv_results_["mean_test_score"], 4), \
  columns = ["acc"])],axis=1))

# Getting roc-auc score for the superior LR classificator..
opt_c = c_space[np.argmax(grid_obj.cv_results_["mean_test_score"])]
opt_clf = LogisticRegression(C = opt_c, max_iter = MAX_ITER, \
  random_state = SEED)
clfs.append({'label': 'LR', 'clf': opt_clf})
res = cross_val_score(opt_clf, xTrain_Val, yTrain_Val, cv = kfold, \
  scoring = SCORE_AUC, n_jobs = -1)
print('Logistic Regression with C = {} -> roc_auc: {:.3}' \
  ' (+/- {:.3})'.format(opt_c, res.mean(), res.std()))

# Now, we are going to use a Random Forest classificator. We start again with
# hyperparameter tuning.
pipelineEstimator = Pipeline([('clf', \
  RandomForestClassifier(random_state = SEED))])
max_depth = [int(x) for x in np.linspace(2, 20, num = 5)]
max_features = ['auto', 'sqrt']
parameters = [{'clf__max_depth': max_depth, \
  'clf__max_features': max_features}]
grid_obj = GridSearchCV(estimator = pipelineEstimator, \
  param_grid = parameters, scoring = SCORE_ACC, n_jobs = -1)
grid_obj.fit(xTrain_Val, yTrain_Val)

print(pd.concat([pd.DataFrame(grid_obj.cv_results_["params"]), \
  pd.DataFrame(np.round(grid_obj.cv_results_["mean_test_score"], 4), \
  columns = ["acc"])],axis=1))

# Getting roc-auc score for the superior RF classificator..
opt_depth = max_depth[np.argmax(grid_obj.cv_results_["mean_test_score"]) // 2]
opt_feat = max_features[np.argmax(grid_obj.cv_results_["mean_test_score"]) % 2]
opt_clf = RandomForestClassifier(max_depth = opt_depth, \
  max_features = opt_feat, random_state = SEED)
clfs.append({'label': 'RF', 'clf': opt_clf})
res = cross_val_score(opt_clf, xTrain_Val, yTrain_Val, cv = kfold, \
  scoring = SCORE_AUC, n_jobs = -1)
print('Random Forest with max_depth = {}, max_features = {} -> roc_auc: {:.3}' \
  ' (+/- {:.3})'.format(opt_depth, opt_feat, res.mean(), res.std()))

# Next, we are going to use an AdaBoost classifier.
pipelineEstimator = Pipeline([('clf', \
  AdaBoostClassifier(random_state = SEED))])
n_estimators = [10, 100, 200, 500, 1000]
learning_rate = [0.01, 0.1]
parameters = [{'clf__n_estimators': n_estimators, \
               'clf__learning_rate': learning_rate}]
grid_obj = GridSearchCV(estimator = pipelineEstimator, \
  param_grid = parameters, scoring = SCORE_ACC, n_jobs = -1, verbose = True)
grid_obj.fit(xTrain_Val, yTrain_Val)

print(pd.concat([pd.DataFrame(grid_obj.cv_results_["params"]), \
  pd.DataFrame(np.round(grid_obj.cv_results_["mean_test_score"], 4), \
  columns = ["acc"])],axis=1))

# Getting roc-auc score for the superior Ada classificator..
opt_est = n_estimators[np.argmax(grid_obj.cv_results_["mean_test_score"]) % 5]
opt_learn = learning_rate[np.argmax(grid_obj.cv_results_["mean_test_score"]) // 5]
opt_clf = AdaBoostClassifier(n_estimators = opt_est, \
  learning_rate = opt_learn, random_state = SEED)
clfs.append({'label': 'Ada', 'clf': opt_clf})
res = cross_val_score(opt_clf, xTrain_Val, yTrain_Val, cv = kfold, \
  scoring = SCORE_AUC, n_jobs = -1)
print('AdaBoost with n_estimators = {}, learning_rate = {} -> roc_auc: {:.3}' \
  ' (+/- {:.3})'.format(opt_est, opt_learn, res.mean(), res.std()))

# Next up is (Gaussian) Naive Bayes.
opt_clf = GaussianNB()
clfs.append({'label': 'GNB', 'clf': opt_clf})
res = cross_val_score(opt_clf, xTrain_Val, yTrain_Val, cv = kfold, \
  scoring = SCORE_ACC, n_jobs = -1)
print('GNB -> accuracy: {:.3}' \
  ' (+/- {:.3})'.format(res.mean(), res.std()))
res = cross_val_score(opt_clf, xTrain_Val, yTrain_Val, cv = kfold, \
  scoring = SCORE_AUC, n_jobs = -1)
print('       roc_auc: {:.3}' \
  ' (+/- {:.3})'.format(res.mean(), res.std()))

# Continuing with Multi-Layer Perceptron..
pipelineEstimator = Pipeline([('clf', \
  MLPClassifier(random_state = SEED, max_iter = MAX_ITER))])
alpha = np.linspace(0.0001, 1, 4)
activation = ['tanh', 'relu', 'logistic']
solver = ['sgd', 'adam', 'lbfgs']
parameters = [{
    'clf__alpha': alpha, \
    'clf__activation': activation, \
    'clf__solver': solver}]
grid_obj = GridSearchCV(estimator = pipelineEstimator, \
  param_grid = parameters, scoring = SCORE_ACC, n_jobs = -1, verbose = True)
grid_obj.fit(xTrain_Val, yTrain_Val)

print(pd.concat([pd.DataFrame(grid_obj.cv_results_["params"]), \
  pd.DataFrame(np.round(grid_obj.cv_results_["mean_test_score"], 4), \
  columns = ["acc"])],axis=1))

# Getting roc-auc score for the superior MLP classificator..
opt_alpha = alpha[np.argmax(grid_obj.cv_results_["mean_test_score"]) % 12 // 3]
opt_activ = activation[np.argmax(grid_obj.cv_results_["mean_test_score"]) // 12]
opt_solver = solver[np.argmax(grid_obj.cv_results_["mean_test_score"]) % 3]
opt_clf = MLPClassifier(alpha = opt_alpha, activation = opt_activ, \
  solver = opt_solver, random_state = SEED, max_iter = MAX_ITER)
clfs.append({'label': 'MLP', 'clf': opt_clf})
res = cross_val_score(opt_clf, xTrain_Val, yTrain_Val, cv = kfold, \
  scoring = SCORE_AUC, n_jobs = -1)
print('MLP with activation = {}, solver = {}, alpha = {} -> roc_auc: {:.3}' \
  ' (+/- {:.3})'.format(opt_activ, opt_solver, opt_alpha,  res.mean(), res.std()))

produceROCs()

# Finally, for the best classificator setup encountered, 
# we will proceed to the testing phase.
best_clf = MLPClassifier(alpha = opt_alpha, activation = opt_activ, \
  solver = opt_solver, random_state = SEED, max_iter = 1000)
best_clf.fit(xTrain_Val,yTrain_Val)
yTrain_Valpred = best_clf.predict(xTrain_Val)
print('Predicting training-validating dataset:\n', \
  classification_report(yTrain_Val, yTrain_Valpred))
yPred = best_clf.predict(xTest)
report = classification_report(yTest, yPred)
print('Predicting testing dataset:\n', report)

# Plotting classification report..
def show_values(pc, fmt="%.2f", **kw):
    pc.update_scalarmappable()
    ax = pc.axes
    for p, color, value in zip(pc.get_paths(), pc.get_facecolors(), \
      pc.get_array()):
        x, y = p.vertices[:-2, :].mean(0)
        if np.all(color[:3] > 0.5):
            color = (0.0, 0.0, 0.0)
        else:
            color = (1.0, 1.0, 1.0)
        ax.text(x, y, fmt % value, ha="center", va="center", color=color, **kw)

def cm2inch(*tupl):
    inch = 2.54
    if type(tupl[0]) == tuple:
        return tuple(i/inch for i in tupl[0])
    else:
        return tuple(i/inch for i in tupl)

def heatmap(AUC, title, xlabel, ylabel, xticklabels, yticklabels, \
  figure_width=40, figure_height=20, correct_orientation=False, cmap='RdBu'):
    fig, ax = plt.subplots()
    c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', \
      linewidths=0.2, cmap=cmap)
    ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)
    ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)
    ax.set_xticklabels(xticklabels, minor=False)
    ax.set_yticklabels(yticklabels, minor=False)
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.xlim( (0, AUC.shape[1]) )
    ax = plt.gca()    
    for t in ax.xaxis.get_major_ticks():
        t.tick1On = False
        t.tick2On = False
    for t in ax.yaxis.get_major_ticks():
        t.tick1On = False
        t.tick2On = False
    plt.colorbar(c)
    show_values(c)
    if correct_orientation:
        ax.invert_yaxis()
        ax.xaxis.tick_top()
    fig = plt.gcf()
    fig.set_size_inches(cm2inch(figure_width, figure_height))

def plot_classification_report(classification_report, \
  title='Classification report ', cmap='RdBu'):
    lines = classification_report.split('\n')
    classes = []
    plotMat = []
    support = []
    class_names = []
    for line in lines[2 : (len(lines) - 4)]:
        t = line.strip().split()
        if len(t) < 2: continue
        classes.append(t[0])
        v = [float(x) for x in t[1: len(t) - 1]]
        support.append(int(t[-1]))
        class_names.append(t[0])
        print(v)
        plotMat.append(v)
    print('plotMat: {0}'.format(plotMat))
    print('support: {0}'.format(support))
    xlabel = 'Metrics'
    ylabel = 'Classes'
    xticklabels = ['Precision', 'Recall', 'F1-score']
    yticklabels = ['{0} ({1})'.format(class_names[idx], sup) for idx, \
      sup  in enumerate(support)]
    figure_width = 25
    figure_height = len(class_names) + 7
    correct_orientation = False
    heatmap(np.array(plotMat), title, xlabel, ylabel, xticklabels, \
      yticklabels, figure_width, figure_height, correct_orientation, cmap=cmap)

plot_classification_report(report)
plt.show()